roles/serviceusage.serviceUsageAdmin
roles/iam.serviceAccountTokenCreator
roles/orgpolicy.policyAdmin at project level
roles/logging.admin
roles/bigquery.admin

Perfect ğŸ‘Œ
Now we build a **production-grade Dataplex module** exactly in your pattern:

âœ” for_each
âœ” map(object())
âœ” CMEK required
âœ” Single RAW zone
âœ” GCS bucket asset
âœ” Labels everywhere
âœ” No IAM bindings
âœ” Bucket already exists
âœ” Region = us-east4

---

# ğŸ“ Final Structure

```
modules/
  dataplex/
    main.tf
    variables.tf
    outputs.tf

root/
  main.tf
  variables.tf

tfvars/
  mics-hcb-dev.tfvars
```

---

# ğŸ“ modules/dataplex/main.tf

```hcl
resource "google_dataplex_lake" "this" {
  for_each = var.lakes

  project      = var.project_id
  location     = each.value.region
  lake_id      = each.key
  display_name = each.value.display_name
  description  = each.value.description

  labels = each.value.labels

  customer_managed_encryption {
    kms_key_name = each.value.kms_key_name
  }
}


resource "google_dataplex_zone" "this" {
  for_each = var.lakes

  project      = var.project_id
  location     = each.value.region
  lake         = google_dataplex_lake.this[each.key].lake_id
  zone_id      = each.value.zone.zone_id
  display_name = each.value.zone.display_name
  description  = each.value.zone.description
  type         = each.value.zone.type

  labels = each.value.zone.labels

  resource_spec {
    location_type = "SINGLE_REGION"
  }

  customer_managed_encryption {
    kms_key_name = each.value.kms_key_name
  }
}


resource "google_dataplex_asset" "this" {
  for_each = var.lakes

  project      = var.project_id
  location     = each.value.region
  lake         = google_dataplex_lake.this[each.key].lake_id
  zone         = google_dataplex_zone.this[each.key].zone_id
  asset_id     = each.value.asset.asset_id
  display_name = each.value.asset.display_name
  description  = each.value.asset.description

  labels = each.value.asset.labels

  resource_spec {
    name = "projects/${var.project_id}/buckets/${each.value.asset.bucket_name}"
    type = "STORAGE_BUCKET"
  }

  discovery_spec {
    enabled = true
  }
}
```

---

# ğŸ“ modules/dataplex/variables.tf

```hcl
variable "project_id" {
  type        = string
  description = "Project ID"
}

variable "lakes" {
  type = map(object({
    region        = string
    display_name  = string
    description   = string
    kms_key_name  = string
    labels        = map(string)

    zone = object({
      zone_id      = string
      display_name = string
      description  = string
      type         = string
      labels       = map(string)
    })

    asset = object({
      asset_id     = string
      display_name = string
      description  = string
      bucket_name  = string
      labels       = map(string)
    })
  }))
}
```

---

# ğŸ“ modules/dataplex/outputs.tf

```hcl
output "lake_names" {
  value = keys(google_dataplex_lake.this)
}

output "zone_names" {
  value = [
    for k, v in google_dataplex_zone.this :
    v.zone_id
  ]
}

output "asset_names" {
  value = [
    for k, v in google_dataplex_asset.this :
    v.asset_id
  ]
}
```

---

# ğŸ“ root/main.tf (ADD MODULE)

```hcl
module "dataplex" {
  source = "./modules/dataplex"

  project_id = var.project_id
  lakes      = var.dataplex_lakes
}
```

---

# ğŸ“ root/variables.tf (ADD VARIABLE)

```hcl
variable "dataplex_lakes" {
  type = map(object({
    region        = string
    display_name  = string
    description   = string
    kms_key_name  = string
    labels        = map(string)

    zone = object({
      zone_id      = string
      display_name = string
      description  = string
      type         = string
      labels       = map(string)
    })

    asset = object({
      asset_id     = string
      display_name = string
      description  = string
      bucket_name  = string
      labels       = map(string)
    })
  }))
}
```

---

# ğŸ“ tfvars/mics-hcb-dev.tfvars

Use SAME KMS key as your other services.

```hcl
dataplex_lakes = {

  "dev-mics-lake" = {
    region       = "us-east4"
    display_name = "Dev MICS Lake"
    description  = "Dataplex lake for dev environment"

    kms_key_name = "projects/cvs-key-vault-nonprod/locations/us-east4/keyRings/gkr-nonprod-us-east4/cryptoKeys/gk-hcb-nonprod-mics-us-east4"

    labels = {
      environment = "dev"
      managed-by  = "terraform"
      application = "mics"
    }

    zone = {
      zone_id      = "dev-raw-zone"
      display_name = "Dev Raw Zone"
      description  = "Raw ingestion zone"
      type         = "RAW"

      labels = {
        environment = "dev"
        managed-by  = "terraform"
      }
    }

    asset = {
      asset_id     = "dev-mics-gcs-asset"
      display_name = "Dev MICS GCS Asset"
      description  = "GCS bucket governed by Dataplex"
      bucket_name  = "dev-mics-ingestion-bucket"

      labels = {
        environment = "dev"
        managed-by  = "terraform"
      }
    }
  }

}
```

---

# ğŸš¨ VERY IMPORTANT â€” CMEK Permission

Grant this ONCE:

```bash
PROJECT_NUMBER=$(gcloud projects describe hcb-nonprod-mics --format="value(projectNumber)")

gcloud kms keys add-iam-policy-binding gk-hcb-nonprod-mics-us-east4 \
  --keyring=gkr-nonprod-us-east4 \
  --location=us-east4 \
  --member="serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-dataplex.iam.gserviceaccount.com" \
  --role="roles/cloudkms.cryptoKeyEncrypterDecrypter"
```

---

# ğŸ”· Required IAM For Infra SA

Infra SA must have:

```
roles/dataplex.admin
```

Without it â†’ Terraform 403 error.

---

# ğŸ”· What Terraform Will Create

âœ” Lake (CMEK encrypted)
âœ” RAW Zone
âœ” GCS Asset attached
âœ” Discovery enabled
âœ” Labels everywhere

Fully enterprise compliant.

---

# ğŸ¯ Now Your Infra Baseline Includes

âœ” GCS
âœ” PubSub
âœ” Artifact Registry (CMEK)
âœ” Secret Manager (CMEK)
âœ” Dataform (CMEK)
âœ” Dataplex (CMEK)

Youâ€™ve basically built a full enterprise data platform foundation.

---

Next logical step from your story:

* Vertex AI API enable?
* API enablement module?
* Service account module?
* IAM module?

Tell me what we tackle next.
